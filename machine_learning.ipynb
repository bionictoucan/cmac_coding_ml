{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5413621-22fb-49ff-b994-9c0f38e9a982",
   "metadata": {},
   "source": [
    "# Solubility Prediction from Molecular Descriptors\n",
    "\n",
    "The following notebook builds upon the dataset introduced and explore in the `data_analysis.ipynb` IPython notebook. Specifically, we load our created data file into this notebook, prepare it for use by two different machine learning algorithms &mdash; random forest and neural network &mdash; and train these two models to find the best model for predicting solubility.\n",
    "\n",
    "As always, we start by importing the relevant Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56633c7d-9185-434b-92f4-7ca23b663179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f774b-4a45-46b1-ac83-8c08368cfb5a",
   "metadata": {},
   "source": [
    "## Data Loading, Splitting and Standardisation\n",
    "\n",
    "The data can be loaded in a similar fashion to the single files in the previous notebook. Note, however, that if you saved the file in the previous notebook as an Excel document then you need to use the `pd.read_excel` function as opposed to the `pd.read_csv` function.\n",
    "\n",
    "### Excercise 1: Loading Data\n",
    "\n",
    "Load the dataset curated during the `data_analysis.ipynb` IPython notebook into a Pandas `DataFrame` and check it is the right data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef404952-a5f2-4f5d-8eda-d50ce3864a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b8644-ebcf-4249-a413-03629b76911b",
   "metadata": {},
   "source": [
    "The relevant data must now be extracted to be used when training and testing the models. This will be accomplished in two steps:\n",
    "\n",
    "1. Extracting the data from the `DataFrame`\n",
    "2. Splitting this data into a training and testing dataset\n",
    "\n",
    "### Exercise 2: Extracting the Data from the `DataFrame`\n",
    "\n",
    "We must extract what we want to be the inputs and the outputs to the ML model from the `DataFrame`. This can be achieved by creating two subsets of the `DataFrame` (as done before in the `data_analysis.ipynb` notebook) one containing only the inputs and one the outputs.\n",
    "\n",
    "Create two variables `X` and `y` where `X` will represent the input to the models and `y` will represent the output to the models.\n",
    "\n",
    "Hint: you can define a list of all the features you want to use in your model and use that when splitting up the `DataFrame` e.g. [here](https://www.ritchieng.com/pandas-scikit-learn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d509a7-a26c-45e8-a775-bab8cca081da",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50559c-9c5c-4baa-bce1-db5b251271eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ?\n",
    "y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966958ff-f22f-4b71-b0b1-16a7a95c8e7e",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Validating and Standardising\n",
    "\n",
    "scikit-learn contains a `train_test_split` function to allow for the easy splitting up of the data. By default this splits the data into 75% training and 25% testing, however we will change this to be 80% training and 20% testing.\n",
    "\n",
    "For more information see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60738829-1415-4388-b377-c8404da3379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487d511-9872-485e-bcd2-86ef02f70a80",
   "metadata": {},
   "source": [
    "Now the data has been split into training and validation data with the validation being 20% of the dataset (defined by argument `test_size=0.2`). `random_state` determines how the data is randomly split between the training and validation set. The idea is that your data will be complete enough that taking a random selection out will not change the data distribution. (We will see later that this is not necessarily the case and there are ways around this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9994c-1e24-4594-b275-bdc18535075b",
   "metadata": {},
   "source": [
    "The data now needs to be standardised as covered in the slides. This is accomplished using `StandardScaler` from scikit-learn. First a `StandardScaler` object must be [defined](https://scikit-learn.org/stable/modules/preprocessing.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663e92e-d7ad-4688-bd36-4d0ec12fa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c46b7f-3806-4adf-a7ed-b5754162a117",
   "metadata": {},
   "source": [
    "Each feature of the data (each `DataFrame` column) is scaled independently, with the assumption that the values for each feature is drawn from the same probability distribution. To find the scaling required for each feature we use the `.fit` function of the `StandardScaler` (it basically works out the mean and standard deviation of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575c948-1cb4-458a-b6fd-f2a842e1e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a2097-4e23-4d3e-b844-e28598c16fee",
   "metadata": {},
   "source": [
    "Note that the `StandardScaler` uses the training data to scale **all** of the data as it is assumed that the validation points will be drawn from the same data distribution as the training points. Now we need to actually scale the points using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac4e1b-85f6-48d7-a767-0186a0256249",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ss.transform(X_train)\n",
    "X_val_scaled = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f5c12-d827-4528-9df0-078caf6c318a",
   "metadata": {},
   "source": [
    "The inputs are not the only data scaled however. It was found that the ML models fit this dataset better when they are logged (see [here](https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html#sphx-glr-auto-examples-compose-plot-transformed-target-py) for some explanation as to why this might be). Therefore we log the outputs of the data using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b1398-ef53-4aaf-97d5-aaada9b9eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled = np.log10(y_train)\n",
    "y_val_scaled = np.log10(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d6d36-4dd6-41e3-9dd3-6d1e3401cedc",
   "metadata": {},
   "source": [
    "## Model Fitting and Predictions\n",
    "\n",
    "The following will now implement two different machine learning models &mdash; random forest and neural network &mdash; to learn to predict solubility from our features.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "A random forest (RF) is a collection of decision trees. A decision tree makes a prediction about a data point based on certain conditions of the features. This can be thought of as a branching path where the tree has a starting condition which splits the data one way or the other. The two subsequent conditions then split the data further and so on and so forth until the response is predicted correctly. e.g.\n",
    "\n",
    "![Decision Tree](images/decision_tree.png)\n",
    "\n",
    "This decision tree is used figure out if an animal is a squirrel, rat, rhino, hippo, elephant or giraffe based on some of its characteristics. This is what we want a decision tree to do. However for data such as ours, the splits that need to be considered are less obvious. Therefore we must try a bunch of different splits of the features and see which gives the best predictions. This is how a decision tree is trained. For a good albeit corny explanation of how decision trees work see the videos [here](https://www.youtube.com/watch?v=ZVR2Way4nwQ) and [here](https://www.youtube.com/watch?v=UhY5vPfQIrA).\n",
    "\n",
    "A random forest is then just a collection of these decision trees which are formulated on a subset of the features. The idea is to get as many predictions as possible and average them to get the prediction from the entire forest (or in the case of classification each tree would get a vote). For a more in-depth explanation of a RF please see the video [here](https://www.youtube.com/watch?v=v6VJ2RO66Ag).\n",
    "\n",
    "Now we will implement the RF algorithm to try to predict our validation data. At the start of the script we imported the `RandomForestRegressor` which is what we will be using for the model (more information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [here](https://scikit-learn.org/stable/modules/ensemble.html#forest)). The main parameter we need to think about when implementing a random forest is the number of decision trees to include in it. By default, scikit-learn uses 100 but we will use 500 (because that's what Tony said to use).\n",
    "\n",
    "\n",
    "### Exercise 1: Defining the Model\n",
    "Change the following line of code to include 500 trees rather than 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55f938-deb8-4bec-8191-d74a8b8e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=100, max_features=\"sqrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cc722-632a-4a78-a66b-3e2ffbb98688",
   "metadata": {},
   "source": [
    "Now the RF must be fit to the data using the `.fit` functions similar to how it works with the `StandardScaler` but this time we need both input and output.\n",
    "\n",
    "### Exercise 2: Fit the Model\n",
    "Fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacced3-4319-4d6a-b8b3-82a7c88d11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860945f3-98db-4ee1-b499-a22ac6ad2260",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluate the Model\n",
    "\n",
    "There are now two functions that we can use to evaluate the performance of the model:\n",
    "\n",
    "* `.predict` which will predict outputs based on inputs passed to it\n",
    "* `.score` will return the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) ($R^{2}$ score) of the model which is just a measure of how well the model predicts unseen data ($R^{2}$ = 1 means the model perfectly predicts all validation data with 0 being the worst it can get).\n",
    "\n",
    "The aim of this exercise is to predict the solubility values of the validation points and compare it with the actual solubility points of the validation points.\n",
    "\n",
    "Using `.predict` will return a list of numbers of predicted solubility, this can then be plotted against the actual solubility to visualise how well our model performs. This plot can be thought of a visualisation of an $R^{2}$ score since the higher the $R^{2}$ score the more like a straight line this plot will look, for more info see [here](https://stats.stackexchange.com/questions/104622/what-does-an-actual-vs-fitted-graph-tell-us).\n",
    "\n",
    "So create a variable `y_pred` to contain the predicted log solubility values and then run the plotting code below to see how well the model fits unseen data. Does it fit well? Do you expect $R^{2}$ to be close to 1, close to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc3378-cd1e-46dd-a114-197f9094df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfr.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c7b3b-7c82-4766-85c1-849ab659e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(y_pred, y_val_scaled)\n",
    "plt.xlabel(\"Predicted Log Solubility\")\n",
    "plt.ylabel(\"Actual Log Solubility\")\n",
    "plt.plot([-4, 3], [-4, 3], \"--k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8da50-ba19-4d4c-bb59-df1183ececa7",
   "metadata": {},
   "source": [
    "To affirm what the plot above tells us, the $R^{2}$ score is calculated. Create a variable called `rf_r2` to store the $R^{2}$ score and then print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dd42f-c968-4c07-bec6-32117e9ded9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r2 = rfr.score(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa12b0-3867-48f8-afe5-da16c6c14d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11db59-b4d6-45a9-9fb6-5789d1aeebab",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "The second type of model we will fit to our data is a deep neural network &mdash; particularly the multilayer perceptron (MLP) model. Neural networks are quite possibly the most well-known ML technique and for good reason &mdash; they are good at learning.\n",
    "\n",
    "The structure of neural networks is relatively easy to understand. They are based on 1950s knowledge of how pathways in the brain work. Consider a neuron in the brain. It will receive some information and then pass an electrical signal based on its response to that information. Now there can be many different neurons interpreting the same information all designed to send different electrical signals to different places. This forms the basis for how neural networks work. In our artificial system each neuron is referred to as a *node* and each set of nodes which processes the same information is called a *layer*. A \"perceptron\" is the original name for a neural network which processes information with a single layer and makes a prediction based on the outputs of each node. Therefore a multilayer perceptron is a modification of this neural network where we use multiple layers. That is, the initial input data is fed into the first layer but then the outputs of the first layer are used as the inputs to the second and so on until an output layer is reached where a prediction is made.\n",
    "\n",
    "How does something like this learn? The data passed to each nodes is transformed via a composition of a linear and non-linear functions. The linear function is parameterised by a set of numbers known as the *learnable parameters*. These learnable parameters start off random with the network making random predictions. The prediction is then compared with the true output value using an MSE metric and this informs the network how to change these learnable parameters to get a better prediction on the output.\n",
    "\n",
    "The actual implementation and maths behind how neural networks work is not important here, what is important is that you understand how to apply a network and how they can be used for predictions. Similar to how we can choose the number of decission trees in a random forest, the number of nodes per layer of a network can be set to give more or less learnable parameters in the system.\n",
    "\n",
    "The following code will be using the `MLPRegressor` from scikit-learn (more information [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)).\n",
    "\n",
    "### Exercise 1: Defining the Model\n",
    "\n",
    "An `MLPRegressor` model can be defined similarly to a `RandomForestRegressor` but this time we would like to set `hidden_layer_sizes = (1000,)` and `max_iter = 10000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efc697-18a8-4ca7-9d4f-f40d1b76e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=?, max_iter=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f36cf3-13a2-46f2-b37e-16b61aa7d828",
   "metadata": {},
   "source": [
    "By default the `MLPRegressor` neural network has **3** layers so setting `hidden_layer_sizes = (1000,)` will construct a 3 layer network where each layer has 1000 nodes.\n",
    "\n",
    "### Exercise 2: Fitting the Model\n",
    "\n",
    "Using the same `.fit` function as for the random forest we can fit our neural network to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f49d2-1d33-4a24-a148-10ba68b519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ec465-5bad-4f37-8808-ff89cad03bbc",
   "metadata": {},
   "source": [
    "### Exercise 3: Evalute the Model\n",
    "\n",
    "And again we can evaluate the model, including the actual vs. predicted plot and the $R^{2}$ score using the `.predict` and `.score` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6f8e4-2976-4607-83e9-68abc843b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8c59e-0887-4752-8812-74e46d1d86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(y_pred, y_val_scaled)\n",
    "plt.xlabel(\"Predicted Log Solubility\")\n",
    "plt.ylabel(\"Actual Log Solubility\")\n",
    "plt.plot([-4, 3], [-4, 3], \"--k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950bc32-2de5-4a15-9ddf-e746faa865d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_r2 = mlp.score(?, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c20be-c0a7-43af-8c35-e08a46afc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b93882-aa4d-450a-8004-9db84b452f0b",
   "metadata": {},
   "source": [
    "### Comparing the Two Models\n",
    "\n",
    "Which model is better? Are you surprised?\n",
    "\n",
    "## Model Improvement\n",
    "\n",
    "The models we have used here are not **amazing**. This could be due to a variety of factors:\n",
    "\n",
    "1. The training dataset is not representative of the true data distribution therefore application to unseen validation is worse.\n",
    "2. The model parameters are not optimal for learning the problem e.g. more/less trees needed, more/less nodes/layers needed.\n",
    "\n",
    "For the last part of the exercise we will focus on 1. ([However, 2. can be addressed by performing a grid search over a bunch of the parameters in the model and seeing which performs the best](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "When we split the data into training and validation using the `train_test_split` function we choose `random_state = 42` randomly. This split up the data into training and validation but may not have split them in an optimal way. That is, the standardisation and model are fitted to the training data so there is an underlying assumption that the training data is representative of the true data distribution. For some splits of the data this may not be the case however and the model may be learning from a bad subset of the data which teaches the model the wrong distribution therefore making the predictions more erroneous.\n",
    "\n",
    "To counter this we perform what is known as *cross validation*. This is when many models are trained on different splits of training and validation data and the model with the average result over all tests used as an indicator of how well our model would perform regardless of dataset permutation. In the code this is implemented through a combination of `KFold` and `cross_validate` to perform *K-Fold cross validation*. K-Fold cross validation just means that the data is split K different times and the model is trained on K subsets of data (typical values are 5, 10, 20). So performing this in the code we must first define a `KFold` object which will tell the `cross_validate` function e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a77538-a1c9-4a7c-b783-585243fc6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052708ca-de4f-43aa-b2aa-f112de95e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=500, max_features=\"sqrt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a00569-56e1-40e6-b3ea-a16d86f77e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log10(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09b91e-29c7-4b86-ab99-6e5bf4731fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cval = cross_validate(pipe, X, y, cv=kf, return_estimator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d3563-39dd-402b-a091-2b8cf321312a",
   "metadata": {},
   "source": [
    "A bit of an explanation for the code above. Since we are doing different dataset splits for each model, the `StandardScaler` must be fitted to each new set of training data. This is where the `make_pipeline` function comes in. This allows us to join together various features in scikit-learn into one workflow to be executed in sequence (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)). Therefore, rather than just fitting a model, we now have the `StandardScaler` applied and then the model is applied. The `cross_validation` function itself expects the model pipeline, the input data, the output data and the K-fold to be used. We set `return_estimator = True` to also return the trained models.\n",
    "\n",
    "`cross_validate` [returns a dictionary](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) with four keys:\n",
    "\n",
    "1. `test_score` &mdash; this is the $R^{2}$ of the model\n",
    "2. `estimator` &mdash; this is a list of the trained models\n",
    "3. `fit_time` &mdash; how long in seconds it took to fit the model\n",
    "4. `score_time` &mdash; how long in seconds it took to score the validation dataset\n",
    "\n",
    "### Exercise 1: Viewing the K-fold Cross Validation Results\n",
    "\n",
    "View this dictionary by printing it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7517639-bb53-4d34-b2f1-90c034441ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d83a41d-af0c-4156-98cd-e7c235a62cbc",
   "metadata": {},
   "source": [
    "Now print just the `test_score` entry of the dictionary to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba34945-a6d3-450c-ac15-85385f854445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd001072-edcc-48c3-b9b5-c125f61c7330",
   "metadata": {},
   "source": [
    "### Exercise 2: Calculate the Mean and Standard Deviation of the Cross Validation Results\n",
    "\n",
    "Hint: Google calculating mean and standard deviation using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4de25-ac00-4754-9348-1da55e6f1dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8e5e74-bb96-4842-98d2-cdada6e67318",
   "metadata": {},
   "source": [
    "### Exercise 3: Do the Same As Above but for the `MLPRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3d3e5-a5c9-473b-8376-3c6510c2fba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
